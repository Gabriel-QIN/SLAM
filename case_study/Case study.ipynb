{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cbe5d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from SLAM_combine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "50f23e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdb_chain(pdb_file, chain='A',pos=None, atom_type='CA', nneighbor=32, cal_cb=True):\n",
    "    \"\"\"\n",
    "    ########## Process PDB file ##########\n",
    "    \"\"\"\n",
    "    current_pos = -1000\n",
    "    X = []\n",
    "    current_aa = {} # N, CA, C, O, R\n",
    "    with open(pdb_file, 'r') as pdb_f:\n",
    "        for line in pdb_f:\n",
    "            if line[21] == chain:\n",
    "                if (line[0:4].strip() == \"ATOM\" and int(line[22:26].strip()) != current_pos) or line[0:4].strip() == \"TER\":\n",
    "                    if current_aa != {}:\n",
    "                        R_group = []\n",
    "                        for atom in current_aa:\n",
    "                            if atom not in [\"N\", \"CA\", \"C\", \"O\"]:\n",
    "                                R_group.append(current_aa[atom])\n",
    "                        if R_group == []:\n",
    "                            R_group = [current_aa[\"CA\"]]\n",
    "                        R_group = np.array(R_group).mean(0)\n",
    "                        X.append([current_aa[\"N\"], current_aa[\"CA\"], current_aa[\"C\"], current_aa[\"O\"], R_group])\n",
    "                        current_aa = {}\n",
    "                    if line[0:4].strip() != \"TER\":\n",
    "                        current_pos = int(line[22:26].strip())\n",
    "\n",
    "                if line[0:4].strip() == \"ATOM\":\n",
    "                    atom = line[13:16].strip()\n",
    "                    if atom != \"H\":\n",
    "                        xyz = np.array([line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]).astype(np.float32)\n",
    "                        current_aa[atom] = xyz\n",
    "    X = np.array(X)\n",
    "    if cal_cb:\n",
    "        X = np.concatenate([X, get_cb(X[:,0], X[:,1], X[:,2])[:, None]], 1)\n",
    "    if pos is not None:\n",
    "        atom_ind = atom_idx[atom_type] # CA atom\n",
    "        if pos >= X.shape[0]:\n",
    "            pos = X.shape[0] - 1\n",
    "        query_coord = X[pos,atom_ind]\n",
    "        distances = calculate_distances(X[:,atom_ind,:], query_coord)\n",
    "        closest_indices = sorted(np.argsort(distances)[:nneighbor])\n",
    "        X = X[(closest_indices)]\n",
    "        # print(closest_indices)\n",
    "    return X, closest_indices # array shape: [Length, 6, 3] N, CA, C, O, R, CB\n",
    "\n",
    "def get_graph_fea_chain(pdb_path, pos, chain='A', nneighbor=32, radius=10, atom_type='CA', cal_cb=True):\n",
    "    X, closest_indices = parse_pdb_chain(pdb_path, chain=chain,pos=pos, atom_type=atom_type, nneighbor=nneighbor, cal_cb=cal_cb)\n",
    "    X = torch.tensor(X).float()\n",
    "    query_atom = X[:, atom_idx[atom_type]]\n",
    "    edge_index = radius_graph(query_atom, r=radius, loop=False, max_num_neighbors=nneighbor, num_workers = 4)\n",
    "    node, edge = get_geo_feat(X, edge_index)\n",
    "    return Data(x=node, edge_index=edge_index, edge_attr=edge, name=os.path.basename(pdb_path).split('.')[0], near=closest_indices)\n",
    "\n",
    "def _get_encoding(seq, feature=[BLOSUM62, BINA]):\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYVX'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    sample = ''.join([re.sub(r\"[UZOB*]\", \"X\", token) for token in seq])\n",
    "    # seq = [char_to_int[char] for char in sample]\n",
    "    max_len = len(sample)\n",
    "    all_fea = []\n",
    "    for encoder in feature:\n",
    "        fea = encoder([sample])\n",
    "        assert fea.shape[0] == max_len\n",
    "        all_fea.append(fea)\n",
    "    return np.hstack(all_fea)\n",
    "    \n",
    "def get_all_inputs(seq, pos, tokenizer, pdb_path):\n",
    "    data = get_graph_fea_chain(pdb_path, pos, nneighbor=32, atom_type='CA', cal_cb=True)\n",
    "    fea = _get_encoding(seq)\n",
    "    s = ''.join([token for token in re.sub(r\"[UZOB*]\", \"X\", seq.rstrip('*'))])\n",
    "    max_len = len(s)\n",
    "    encoded = tokenizer.encode_plus(seq, add_special_tokens=True, padding='max_length', return_token_type_ids=False, pad_to_max_length=True,truncation=True, max_length=max_len, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    return data, input_ids, attention_mask, torch.tensor(fea, dtype=torch.float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3bac5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_encoding(seq, feature=[BLOSUM62, BINA]):\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYVX'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    sample = ''.join([re.sub(r\"[UZOB*]\", \"X\", token) for token in seq])\n",
    "    # seq = [char_to_int[char] for char in sample]\n",
    "    max_len = len(sample)\n",
    "    all_fea = []\n",
    "    for encoder in feature:\n",
    "        fea = encoder([sample])\n",
    "        assert fea.shape[0] == max_len\n",
    "        all_fea.append(fea)\n",
    "    return np.hstack(all_fea)\n",
    "\n",
    "class SLAMPredDataset(object):\n",
    "    def __init__(self, peplist, tokenizer, pdb_path):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_list = []\n",
    "        self.graphlist = []\n",
    "        self.seqlist = []\n",
    "        self.label_list = []\n",
    "        for record in tqdm(peplist):\n",
    "            seq = record[-1]\n",
    "            desc = record[0].split('|')\n",
    "            name, pos, length = desc[0], int(desc[2]), int(desc[3])\n",
    "            data = get_graph_fea(pdb_path, pos, nneighbor=32, atom_type='CA', cal_cb=True)\n",
    "            self.graphlist.append(data)\n",
    "            fea = _get_encoding(seq)\n",
    "            self.feature_list.append(fea)\n",
    "            self.seqlist.append(seq)\n",
    "            self.label_list.append(desc)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphlist)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqlist[index]\n",
    "        seq = [token for token in re.sub(r\"[UZOB*]\", \"X\", seq.rstrip('*'))]\n",
    "        max_len = len(seq)\n",
    "        encoded = self.tokenizer.encode_plus(' '.join(seq), add_special_tokens=True, padding='max_length', return_token_type_ids=False, pad_to_max_length=True,truncation=True, max_length=max_len, return_tensors='pt')\n",
    "        input_ids = encoded['input_ids'].flatten()\n",
    "        attention_mask = encoded['attention_mask'].flatten()\n",
    "        self.graphlist[index], input_ids, attention_mask, torch.tensor(self.feature_list[index], dtype=torch.float), self.label_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "64572803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peptide(pos, window_size, seq, mirror=True):\n",
    "    \"\"\"Return peptide based on window_size. Missing residues are padded with X symbol (if mirror == False) or mirroring residues from the other side (if mirror == True).\"\"\"\n",
    "    pos = pos-1\n",
    "    half_window = int(window_size/2)\n",
    "    start = pos - half_window\n",
    "    left_padding = '' if start >= 0 else 'X' * abs(start)\n",
    "    start = 0 if start < 0 else start\n",
    "    end = pos + half_window + 1\n",
    "    right_padding = 'X' * half_window\n",
    "    end = len(seq) if end + 1 > len(seq) else end\n",
    "    peptide_ = seq[start:end]\n",
    "    if mirror:\n",
    "        if left_padding == '' and right_padding == '':\n",
    "            peptide = left_padding + peptide_ + right_padding\n",
    "        elif left_padding == '' and right_padding != '': # mirror left\n",
    "            peptide = left_padding + peptide_ + peptide_[:len(right_padding)][::-1]\n",
    "        elif left_padding != '' and right_padding == '': # mirror right\n",
    "            peptide = peptide_[::-1][:len(left_padding)] + peptide_ + right_padding\n",
    "        else:\n",
    "            peptide = None\n",
    "    else:\n",
    "        peptide = left_padding + peptide_ + right_padding\n",
    "    if peptide is not None:\n",
    "        peptide = peptide[:window_size]\n",
    "        assert peptide[half_window] == 'K' and len(peptide) == window_size\n",
    "        return peptide\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6b4f0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_k(seqlist, window_size=51):\n",
    "    peplist = []\n",
    "    window_size = window_size\n",
    "    half_window = window_size // 2\n",
    "    for record in seqlist:\n",
    "        seq = str(record.seq)\n",
    "        for m in re.finditer('K', seq):\n",
    "            pos = m.start() + 1\n",
    "            pep = get_peptide(pos, window_size, seq, mirror=False)\n",
    "            if pep is not None:\n",
    "                peplist.append([f'{pdb_id}|Pred|{pos}|{len(seq)}', pep])\n",
    "    return peplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4b6474e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_id = '5w49'\n",
    "pdb_path = f'proteomes/{pdb_id}.pdb'\n",
    "seq_path = f'proteomes/{pdb_id}.fa'\n",
    "\n",
    "pretrained_model = '/mnt/data/zhqin/pretrain_LM/prot_bert/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, do_lower_case=False, use_fast=False)\n",
    "window_size = 51\n",
    "seqlist = [record for record in SeqIO.parse(seq_path, \"fasta\")]\n",
    "peplist = get_all_k(seqlist, window_size=window_size)\n",
    "# pep_ds = SLAMPredDataset(peplist, tokenizer, pdb_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "da918bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KLPYKVADIGLAAWGRKALDIAENEMPGLMRMRERYSASKPLKGARIAGCLHMTVETAVLIETLVTLGAEVQWSSCNIFSTQDHAAAAIAKAGIPVYAWKGETDEEYLWCIEQTLYFKDGPLNMILDDGGDLTNLIHTKYPQLLPGIRGISEETTTGVHNLYKMMANGILKVPAINVNDSVTKSKFDNLYGCRESLIDGIKRATDVMIAGKVAVVAGYGDVGKGCAQALRGFGARVIITEIDPINALQAAMEGYEVTTMDEACQEGNIFVTTTGCIDIILGRHFEQMKDDAIVCNIGHFDVEIDVKWLNENAVEKVNIKPQVDRYRLKNGRRIILLAEGRLVNLGCAMGHPSFVMSNSFTNQVMAQIELWTHPDKYPVGVHFLPKKLDEAVAEAHLGKLNVKLTKLTEKQAQYLGMSCDGPFKPDHYRY'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(seqlist[0].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "794b3b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 XXXXXXXXXXXXXXXXXXXXXXXXXKLPYKVADIGLAAWGRKALDIAENEM\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "for desc, seq in peplist:\n",
    "    seq = str(seq)\n",
    "    tmp = desc.split('|')\n",
    "    pos = int(tmp[2])\n",
    "    print(pos,seq)\n",
    "    g, input_ids, attention_mask, feature = get_all_inputs(seq,pos,tokenizer,pdb_path)\n",
    "    feature = feature.unsqueeze(0).to(device)\n",
    "    g = g.to(device)\n",
    "    g.batch = torch.zeros(g.x.shape[0],dtype=torch.int64).to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    pred = model(input_ids=input_ids, attention_mask=attention_mask, feature=feature, g_data=g)\n",
    "    result = [seq,]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "514bede2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0022]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " 'XXXXXXXXXXXXXXXXXXXXXXXXXKLPYKVADIGLAAWGRKALDIAENEM')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6a71dc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MENEAIDLAKRGWAALGIDAVKYPL'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(reversed('LPYKVADIGLAAWGRKALDIAENEM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c516fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(pep_ds,batch_size=len(pep_ds),shuffle=False,num_workers=8, collate_fn=graph_collate_fn, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "63f896c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/data/zhqin/pretrain_LM/prot_bert/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "n_layers = 1\n",
    "dropout = 0.5\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "out_dim = 32\n",
    "\n",
    "node_dim = 267\n",
    "edge_dim = 632\n",
    "nneighbor = 32\n",
    "atom_type = 'CA' # CB, R, C, N, O\n",
    "gnn_layers = 5\n",
    "encoder_list = ['cnn','lstm','fea', 'gnn', 'plm']\n",
    "fea_dim = 41\n",
    "PLM_dim = 1024\n",
    "BERT_encoder = AutoModel.from_pretrained(pretrained_model, local_files_only=True, output_attentions=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a8723ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = f'Models/SLAM_combine/general_struct_plm/best_general_struct_plm_model_epoch.pt'  \n",
    "model = SLAMNet(BERT_encoder=BERT_encoder, vocab_size=tokenizer.vocab_size, encoder_list=encoder_list,PLM_dim=PLM_dim,win_size=window_size,embedding_dim=embedding_dim, fea_dim=fea_dim, hidden_dim=hidden_dim, out_dim=out_dim,node_dim=node_dim, edge_dim=edge_dim, gnn_layers=gnn_layers,n_layers=n_layers,dropout=dropout).to(device)\n",
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b7b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111a53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
